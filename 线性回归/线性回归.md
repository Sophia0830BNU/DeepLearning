# 线性回归

### 梯度下降
$\textbf{梯度}$： 一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）  
$w_t = w_{t-1} - \eta \frac{\partial l}{\partial w_{t-1}}$  
$\eta$为学习率，即步长的超参数

同时，一个深度神经网络模型可能需要数分钟至数小时。我们可以随机采样b个样本 来近似损失： 
$\frac{1}{b} \sum_{i\in{I_b}}l(x_i,y_i,w)$  

具体实现时， 使用backward() 求梯度需要为标量，因此我们通常先计算小批量的损失和为标量，平均值可在之后除以batchsize 或 记录在梯度中。

```python
## 定义优化算法(梯度下降)
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr *param.grad / batch_size
            # 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值
            param.grad.zero_() #清零， 下次计算不受影响

```

训练过程：

```python
for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        #计算[w, b]的梯度 求和成为标量，使其能够访问梯度，
        l.sum().backward()
        ## 前面若没取平均可在此处取 (l.sum()/batch_size).backward()
        sgd([w, b], lr, batch_size)

    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch{epoch+1}, loss{float(train_l.mean()):f}')
```

$\textbf{总结}$：  
1、梯度下降通过不断沿着反梯度方向更新参数  
2、小批量随机梯度下降是深度学习默认的求解算法
3、两个重要的超参数是批量大小和学习率  