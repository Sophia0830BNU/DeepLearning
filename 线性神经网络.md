# 线性回归

### 梯度下降
$\textbf{梯度}$： 一个向量（矢量），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）  
$w_t = w_{t-1} - \eta \frac{\partial l}{\partial w_{t-1}}$  
$\eta$为学习率，即步长的超参数

同时，一个深度神经网络模型可能需要数分钟至数小时。我们可以随机采样b个样本 来近似损失： 
$\frac{1}{b} \sum_{i\in{I_b}}l(x_i,y_i,w)$  

$\textbf{总结}$：  
1、梯度下降通过不断沿着反梯度方向更新参数  
2、小批量随机梯度下降是深度学习默认的求解算法
3、两个重要的超参数是批量大小和学习率  
